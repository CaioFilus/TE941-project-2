# -*- coding: utf-8 -*-
"""CÃ³pia de MNIST in Keras.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16XV4Z-qEP2K0l9BCNDr4TxOI3aYeHvOf
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

"""## Prerequisite Python Modules

First, some software needs to be loaded into the Python environment.
"""

import numpy as np                   # advanced math library
import matplotlib.pyplot as plt      # MATLAB like plotting routines
import random                        # for generating random numbers

from keras.datasets import mnist     # MNIST dataset is included in Keras
from keras.models import Sequential  # Model type to be used

from keras.layers.core import Dense, Dropout, Activation # Types of layers to be used in our model
from keras.utils import np_utils                         # NumPy related tools

# The MNIST data is split between 60,000 28 x 28 pixel training images and 10,000 28 x 28 pixel images
(X_train, y_train), (X_test, y_test) = mnist.load_data()

print("X_train shape", X_train.shape)
print("y_train shape", y_train.shape)
print("X_test shape", X_test.shape)
print("y_test shape", y_test.shape)

plt.rcParams['figure.figsize'] = (9,9) # Make the figures a bit bigger

for i in range(9):
    plt.subplot(3,3,i+1)
    num = random.randint(0, len(X_train))
    plt.imshow(X_train[num], cmap='gray', interpolation='none')
    plt.title("Class {}".format(y_train[num]))
    
plt.tight_layout()

# just a little function for pretty printing a matrix
def matprint(mat, fmt="g"):
    col_maxes = [max([len(("{:"+fmt+"}").format(x)) for x in col]) for col in mat.T]
    for x in mat:
        for i, y in enumerate(x):
            print(("{:"+str(col_maxes[i])+fmt+"}").format(y), end="  ")
        print("")

# now print!        
matprint(X_train[num])



X_train = X_train.reshape(60000, 784) # reshape 60,000 28 x 28 matrices into 60,000 784-length vectors.
X_test = X_test.reshape(10000, 784)   # reshape 10,000 28 x 28 matrices into 10,000 784-length vectors.

X_train = X_train.astype('float32')   # change integers to 32-bit floating point numbers
X_test = X_test.astype('float32')

X_train /= 255                        # normalize each value for each pixel for the entire vector for each input
X_test /= 255

print("Training matrix shape", X_train.shape)
print("Testing matrix shape", X_test.shape)



nb_classes = 10 # number of unique digits

Y_train = np_utils.to_categorical(y_train, nb_classes)
Y_test = np_utils.to_categorical(y_test, nb_classes)

# The Sequential model is a linear stack of layers and is very common.

modelAcurracy = Sequential()
# The first hidden layer is a set of 512 nodes (artificial neurons).
# Each node will receive an element from each input vector and apply some weight and bias to it.
modelAcurracy.add(Dense(512, input_shape=(784,))) #(784,) is not a typo -- that represents a 784 length vector!
# An "activation" is a non-linear function applied to the output of the layer above.
# It checks the new value of the node, and decides whether that artifical neuron has fired.
# The Rectified Linear Unit (ReLU) converts all negative inputs to nodes in the next layer to be zero.
# Those inputs are then not considered to be fired.
# Positive values of a node are unchanged.
modelAcurracy.add(Activation('relu'))
# Dropout zeroes a selection of random outputs (i.e., disables their activation)
# Dropout helps protect the model from memorizing or "overfitting" the training data.
modelAcurracy.add(Dropout(0.2))
# The second hidden layer appears identical to our first layer.
# However, instead of each of the 512-node receiving 784-inputs from the input image data,
# they receive 512 inputs from the output of the first 512-node layer.
modelAcurracy.add(Dense(512))
modelAcurracy.add(Activation('relu'))
modelAcurracy.add(Dropout(0.2))
# The final layer of 10 neurons in fully-connected to the previous 512-node layer.
# The final layer of a FCN should be equal to the number of desired classes (10 in this case).
modelAcurracy.add(Dense(10))
# The "softmax" activation represents a probability distribution over K different possible outcomes.
# Its values are all non-negative and sum to 1.
modelAcurracy.add(Activation('softmax'))


modelAcurracy.compile(loss='categorical_crossentropy', metrics=['accuracy'])

modelAcurracy.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=1)

modelMSE = Sequential()

modelMSE.add(Dense(512, input_shape=(784,)))
modelMSE.add(Activation('relu'))
modelMSE.add(Dropout(0.2))

modelMSE.add(Dense(512))
modelMSE.add(Activation('relu'))
modelMSE.add(Dropout(0.2))

modelMSE.add(Dense(10))
modelMSE.add(Activation('softmax'))

modelMSE.compile(loss='categorical_crossentropy', metrics=['MeanSquaredError'])

modelMSE.fit(X_train, Y_train, batch_size=128, epochs=5, verbose=1)



scoreAcurracy = modelAcurracy.evaluate(X_test, Y_test)
scoreMSE = modelMSE.evaluate(X_test, Y_test)
print('Acurracy Test score:', scoreAcurracy[0],'Test accuracy:', scoreAcurracy[1] )
print('MSE Test score:', scoreMSE[0],'Test accuracy:', scoreMSE[1] )